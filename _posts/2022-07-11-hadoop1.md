---
title:  "Mastering Hadoop3 Chapter1-2"
excerpt: ""
categories:
  - Book

toc: true
toc_sticky: true
---

# Chapter 1

하둡 전의 문제? 하둡은 이 문제를 어떻게 해결했는가. 
아파치 루씬(정보 검색, 색인 및 검색) > 아파치 너치(검색엔진, 웹 크롤러 HTML 구문 분석)  
scalability의 부족.  
Doug와 Mike는(루씬 개발자) 3가지 특징을 가지는 시스템을 원했음

1. 내결함성
2. 로드밸런싱
3. 데이터 손실(고가용성)

이 시점에 구글에서 GFS 발표, **NDFS***(Nutch Distribuited File System)* 개발 시작(2004년)  
block과 replication 개념을 이용.   
Block은 파일을 64MB chunks(사이즈 조정가능) 로 분해하고 그 블록을 3번 복사.

## 맵 리듀스

NDFS 에 저장된 데이터를 처리 할 수 있는 알고리즘을 개발.  
기계의 개수를 두배로 늘리면 두배의 성능이 나오는것을 기대함.  
그 시기에 구글이 MapReduce를 배포함. 

**맵 리듀스의 특성**

1. Parallelism (평행성)
2. 내결함성
3. Data Locality 
   -  (프로그램이 있는 곳에 데이터를 가져오는 것이 아닌 데이터가 있는곳에서 프로그램을 실행)

맵리듀스는 2005년에 넛치에 통합되었다.  
2006년에 **HDFS***(Hadoop Distributed File System)* 프로젝트를 시작 했으며   
이는 NDFS, MapReduce, Hadoop Common 등의 이름을 따서 지어졌다.

네임노드: 메타데이터 정보를 관리.  
MapReduce 엔진: Job tracker, Task tracker, 확장성이 40,000 노드 까지밖에 안됌.  
이 문제를 해결하기 위해 YARN이 하둡2에 등장 > 확장성 문제와 자원관리 job 을 극복.

## 하둡 3의 원동력

- 버그 수정, 성능향상
- 오픈 소스 커뮤니티에서 많은 수정사항이 있었고 이를 다 반영하기 위해 하둡3로 릴리즈
- 데이터 복제 팩터로 인한 오버헤드
- **erasure coding**을 이용해 해결 (데이터 사이즈를 잘 결정해야 namenode에 부하 x)
    
    [Hadoop 3에서 Erasure Coding이란 - 2](https://joonyon.tistory.com/entry/Hadoop-3%EC%97%90%EC%84%9C-Erasure-Coding%EC%9D%B4%EB%9E%80-2?category=760357)
    
- YARN Timeline service 향상
- NameNode 고가용성 향상
- 하둡의 Default 포트 그대로 사용
- HDFS diskbalncer

## 하둡 플랫폼 Logical view

<p align="center"><img src="/assets/images/hadoop/hadoop_logical.png" width=400></p>

- **Ingress/egress/processing -** 자동화 가능
  - Ingesting data
  - Reading data
  - Processing already ingested data
- **Data integration components**  
스쿱, JAVA Hadoop Client 등..
- **Data access interfaces**  
다양한 언어로 하둡 데이터에 접근할수 있게 해줌.
- **Data Processing Engines**  
하둡에 저장된 데이터를 연산할 수 있는 다양한 엔진.
MapReduce 는 RAM 메모리를 보존하고 디스크 I/O를 더 쓰는 점에서 배치 데이터 처리에 적합하고, Spark 는 disk I/O-bound 가 적고 램 메모리를 더 사용하는 점에서 실시간 데이터나 micro-batch 처리에 적합하다.
- **Resource management frameworks**  
Hadoop의 작업 및 작업 스케줄링을 위해 추상 API를 노출하여 기본 리소스 관리자(YARN, Mesos)와 상호 작용
- **Task and resource management**  
클러스터 내에서 동시에 실행되는 서로 다른 애플리케이션에서 대규모 클러스터 머신을 공유  
YARN은 Unix process.  MESOS는 Linux-container-based.
- **Data input/output**  
- **Data Storage Medium**  
HDFS: JAVA 기반, 유닉스 파일시스템  

*****

# Chapter 2
하둡 HDFS에 대해 자세히 알아보자

## HDFS의 특징  

1. 내결함성
2. 실시간 데이터 접근
3. 확장성
4. 간단함
5. 고가용성

하둡 플랫폼은 **분산 저장소**와 **분산 처리**, 두개의 논리적 구성으로 되어있다.  
_HDFS_ 가 **분산 저장**소 를, _MapReduce와 YARN-호환 프레임워크들_ 이 **분산 처리** 능력을 제공한다.  

## HDFS logical architecture

<p align="center"><img src="/assets/images/hadoop/chapter2_1.png" width=600></p>

이 아키텍쳐는 간단하게 **데이터 그룹**과 **관리 그룹**, 두가지 그룹으로 나눌 수 있다.  
**데이터 그룹**은 파일 저장소의 처리/구성요소를 포함하고   
**관리 그룹**은 데이터 오퍼레이션(읽기,쓰기,자르기,삭제)을 관리하는 처리/구성요소를 포함한다.  
<br>
데이터 그룹: Data blocks, replication, checkpoints, file metadata  
관리 그룹: NameNodes, DataNodes, JournalNodes, Zookeepers  
<br>

### 관리 그룹(Management group)의 구성요소

- **NameNode**: HDFS는 마스터-슬레이브 구조이고 네임노드는 마스터의 역할을 한다. 네임노드는 데이터의 메타데이터를 관리하는데 File System namespace, fsimage files, edit logs files 이렇게 세가지의 정보를 저장한다.  
fsimage는 파일 시스템의 한 시점의 상태를 저장  
edit log는 fsimage가 저장된 이후의 HDFS의 모든 파일의 변화를 저장.  
fsimage와 edit log를 병합하는것을 체크포인팅이라고 한다.

- **DataNode**: HDFS의 구조에서 슬레이브 역할을 한다. 네임노드나 HDFS 클라이언트로부터 받은 data block operation을 수행(생성, 수정, 삭제)  
processing job은 MapReduce등을 이용하고 block의 정보를 네임노드에 다시 전달한다. 또한 복제가 있는 데이터 노드끼리 통신 할 수 있다.  

- **JournalNode**: 저널노드는 활성된 네임노드와 대기중인 네임노드 사이에서 edit log와 HDFS 메타데이터를 관리하는 역할이다.  
쓰기 동시성 제어를 통해 활성된 하나의 네임노드가 edit log를 작성하도록 한다.

- **Zookeeper failover controllers(ZKFC)**: 하둡은 네임노드의 고가용성을 유지하기 위해 두가지 구성요소를 소개한다 **Zookeeper Quorum**과 **ZKFC**이다.  
주키퍼는 네임노드의 health와 connectivity의 정보를 점검하며 네임노드가 만료되면 재시작하는 역할을 수행한다.  
failure이나 crash 이벤트가 발생한다면 만료된 세션은 failed 네임노드에 의해 재시작 되지 못하며 이때 주키퍼가 대기중인 네임노드에 failover 프로세스를 시작한다. 이 주키퍼의 클라이언트가 ZKFC이며 네임노드의 건강을 모니터링하고 세션을 관리하며 쓰기 동시성 제어를 한다.

### 데이터 그룹(Data group)의 개념
데이터 그룹은 **Block**, **Replication** 두개의 개념이 있다.  
1. Block은 HDFS가 한번에 읽거나 쓸수 있는 최소 데이터의 양을 정의한다.  
기본 block의 크기는 64MB나 128MB 이며 이는 Unix-level 파일시스템과 비교 했을 때 더 큰 사이즈다.  
이렇게 큰 블록 사이즈를 사용하는 이유가 여러가지 있는데, **첫번째**로 만약 블록의 사이즈가 너무 작다면 네임노드가 관리해야하는 메타데이터가 많아져 메모리가 모자라는 현상이 발생한다. 큰 사이즈의 블록은 메타데이터를 효과적으로 관리할 수 있다.  
**두번째**로 데이터 블록이 클 경우 Hadoop의 처리량이 증가한다.

2. Replication 

<p align="center"><img src="/assets/images/hadoop/chapter2_2.png" width=600></p>